{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCcLTkzdc/lB+O0x+l9J9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coder-Abhishek05/NLP/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Lowercasing and removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "sentiment_score = sid.polarity_scores(text)\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokenized Text:\", tokens)\n",
        "print(\"Filtered Tokens (Lowercased, Stopwords Removed):\", filtered_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "print(\"Sentiment Score:\", sentiment_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtJjda0cEqnP",
        "outputId": "0c28530f-a2f7-46f4-b869-6cb0498001f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLTK is a leading platform for building Python programs to work with human language data.\n",
            "Tokenized Text: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "Filtered Tokens (Lowercased, Stopwords Removed): ['nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', '.']\n",
            "Lemmatized Tokens: ['nltk', 'leading', 'platform', 'building', 'python', 'program', 'work', 'human', 'language', 'data', '.']\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the IMDb movie review dataset\n",
        "try:\n",
        "    from nltk.corpus import movie_reviews\n",
        "    nltk.download('movie_reviews')\n",
        "except LookupError:\n",
        "    nltk.download('movie_reviews')\n",
        "    from nltk.corpus import movie_reviews\n",
        "\n",
        "# Create DataFrame from the movie reviews\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "df = pd.DataFrame(documents, columns=['words', 'category'])\n",
        "\n",
        "# Combine words to form sentences\n",
        "df['text'] = df['words'].apply(' '.join)\n",
        "df.drop(columns=['words'], inplace=True)\n",
        "\n",
        "# Text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = pipeline.predict(X_train)\n",
        "y_pred_test = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "print(\"Classification Report for Test Data:\")\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usopZXkhH4D0",
        "outputId": "5bbd41ca-996e-4188-febf-2cabec75bde9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.984375\n",
            "Test Accuracy: 0.815\n",
            "Classification Report for Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.81      0.81       199\n",
            "         pos       0.82      0.82      0.82       201\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the trained model\n",
        "try:\n",
        "    from nltk.corpus import movie_reviews\n",
        "    nltk.download('movie_reviews')\n",
        "except LookupError:\n",
        "    nltk.download('movie_reviews')\n",
        "    from nltk.corpus import movie_reviews\n",
        "\n",
        "# Load the IMDb movie review dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "df = pd.DataFrame(documents, columns=['words', 'category'])\n",
        "\n",
        "# Combine words to form sentences\n",
        "df['text'] = df['words'].apply(' '.join)\n",
        "df.drop(columns=['words'], inplace=True)\n",
        "\n",
        "# Text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str) and text.strip():  # Check if text is a non-empty string\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Define the model\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
        "y = df['category']\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X, y)\n",
        "\n",
        "# Read input from Excel file\n",
        "input_file_path = 'flipkart.xlsx'\n",
        "output_file_path = 'output_data.xlsx'\n",
        "\n",
        "input_df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Apply sentiment analysis to each input\n",
        "# Apply sentiment analysis to each input\n",
        "sentiments = []\n",
        "for index, row in input_df.iterrows():\n",
        "    text = row['Text']\n",
        "    processed_text = preprocess_text(text)\n",
        "    if processed_text:  # Check if processed text is non-empty\n",
        "        text_tfidf = tfidf_vectorizer.transform([processed_text])\n",
        "        sentiment = classifier.predict(text_tfidf)[0]\n",
        "    else:\n",
        "        sentiment = 'Unknown'  # Handle cases where text is empty or non-string\n",
        "    sentiments.append(sentiment)\n",
        "\n",
        "\n",
        "input_df['Sentiment'] = sentiments\n",
        "\n",
        "# Save output to Excel file\n",
        "input_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(\"Sentiments have been successfully analyzed and saved to\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaD_bWYmJgDb",
        "outputId": "7c3ec4f0-6206-4d49-ddef-95a672f425d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiments have been successfully analyzed and saved to output_data.xlsx\n"
          ]
        }
      ]
    }
  ]
}